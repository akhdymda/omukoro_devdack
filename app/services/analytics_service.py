import logging
import re
from typing import List, Optional, Dict, Tuple, Any
from collections import Counter
from datetime import datetime
from app.models.analysis import AnalyticsRequest, AnalyticsResponse, ConsultantInfo, AnalysisMetadata
from app.services.dummy_data_service import DummyDataService
# from app.services.rag_service import RAGService

logger = logging.getLogger(__name__)

class AnalyticsService:
    """
    è«–ç‚¹ãƒ»è³ªå•äº‹é …ã¨ç›¸è«‡å…ˆã®åˆ†æã‚µãƒ¼ãƒ“ã‚¹
    
    RAGç›¸å½“ã®æ¨è«–æ©Ÿèƒ½ã‚’å®Ÿè£…ï¼ˆãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰
    å°†æ¥çš„ã«RAGã‚·ã‚¹ãƒ†ãƒ ã«å·®ã—æ›¿ãˆäºˆå®š
    """
    
    def __init__(self):
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚µãƒ¼ãƒ“ã‚¹ã®ã¿ä½¿ç”¨
        self.dummy_service = DummyDataService()

        # RAG ã¯ç„¡åŠ¹åŒ–
        self.rag_service = None
        self.use_rag = False
        logger.info("RAG is disabled for deployment")
    
    async def analyze_consultation(self, request: AnalyticsRequest) -> AnalyticsResponse:
        """
        æœ¬æ ¼çš„ãªRAGã¾ãŸã¯RAGé¢¨æ¨è«–ã«ã‚ˆã‚‹ç›¸è«‡å†…å®¹åˆ†æ
        
        Args:
            request: åˆ†æãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            
        Returns:
            AnalyticsResponse: åˆ†æçµæœ
        """
        try:
            logger.info(f"ğŸš€ åˆ†æé–‹å§‹: ãƒ†ã‚­ã‚¹ãƒˆé•· {len(request.text)}, ãƒ•ã‚¡ã‚¤ãƒ«æ•° {len(request.files_content or [])}")
            
            # å…¥åŠ›æ¤œè¨¼
            if not self._validate_input(request):
                raise ValueError("ç›¸è«‡å†…å®¹ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            
            # ğŸ†• RAGã‚µãƒ¼ãƒ“ã‚¹åˆ©ç”¨å¯èƒ½ãªå ´åˆ
            if self.use_rag and self.rag_service:
                logger.info("ğŸ§  æœ¬æ ¼çš„ãªRAGã‚·ã‚¹ãƒ†ãƒ ã§åˆ†æå®Ÿè¡Œ")
                rag_result = await self.rag_service.analyze_with_rag(
                    text=request.text,
                    files_content=request.files_content or []
                )
                return self._convert_rag_result_to_response(rag_result)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®RAGé¢¨æ¨è«–
            logger.info("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: RAGé¢¨æ¨è«–ã§åˆ†æå®Ÿè¡Œ")
            
            # ğŸ” Step 1: ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã¨ç‰¹å¾´æŠ½å‡º
            processed_data = self._preprocess_and_extract_features(request)
            
            # ğŸ§  Step 2: RAGé¢¨å¤šæ®µéšæ¨è«–
            reasoning_result = await self._rag_reasoning_pipeline(processed_data)
            
            # ğŸ“Š Step 3: ä¿¡é ¼åº¦ã®å‹•çš„è¨ˆç®—
            confidence_score = self._calculate_dynamic_confidence(processed_data, reasoning_result)
            
            # ğŸ“ Step 4: æ¨è«–éç¨‹ã®è¨˜éŒ²
            reasoning_trace = self._generate_reasoning_trace(processed_data, reasoning_result)
            
            # ğŸ¯ Step 5: æœ€çµ‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹ç¯‰
            response = self._build_final_response(
                reasoning_result, 
                confidence_score, 
                reasoning_trace
            )
            
            logger.info(f"âœ… åˆ†æå®Œäº†: è«–ç‚¹{len(response.questions)}ä»¶, ç›¸è«‡å…ˆ{len(response.consultants)}å")
            return response
            
        except ValueError as e:
            logger.warning(f"âŒ Analyticså…¥åŠ›ã‚¨ãƒ©ãƒ¼: {e}")
            raise
        except Exception as e:
            logger.error(f"ğŸ’¥ Analyticsæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            raise Exception("åˆ†æå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
    
    def _convert_rag_result_to_response(self, rag_result: Dict[str, Any]) -> AnalyticsResponse:
        """RAGçµæœã‚’AnalyticsResponseã«å¤‰æ›"""
        try:
            consultants = [
                ConsultantInfo(
                    name=consultant["name"],
                    department=consultant["department"],
                    expertise=consultant["expertise"]
                )
                for consultant in rag_result.get("consultants", [])
            ]
            
            metadata = AnalysisMetadata(
                confidence=rag_result["analysis_metadata"]["confidence"],
                generated_at=rag_result["analysis_metadata"]["generated_at"],
                used_dummy_data=rag_result["analysis_metadata"].get("used_dummy_data", True),
                matched_regulations=rag_result["analysis_metadata"].get("matched_regulations", []),
                fallback=rag_result["analysis_metadata"].get("fallback", False)
            )
            
            return AnalyticsResponse(
                summary=rag_result["summary"],
                questions=rag_result["questions"],
                consultants=consultants,
                analysis_metadata=metadata
            )
            
        except Exception as e:
            logger.error(f"âŒ RAGçµæœå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            # æœ€å°é™ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return AnalyticsResponse(
                summary="ä¼ç”»æ¡ˆã«ã¤ã„ã¦æ³•ä»¤é©åˆæ€§ã®ç¢ºèªãŒå¿…è¦ã§ã™ã€‚",
                questions=["é–¢é€£æ³•ä»¤ã®é©ç”¨ç¯„å›²ç¢ºèª", "å¿…è¦ãªè¨±èªå¯ã®ç‰¹å®š"],
                consultants=[
                    ConsultantInfo(
                        name="ç”°ä¸­ æ³•å‹™",
                        department="æ³•å‹™éƒ¨",
                        expertise="é…’ç¨æ³•ãƒ»é£Ÿå“è¡›ç”Ÿæ³•"
                    )
                ],
                analysis_metadata=AnalysisMetadata(
                    confidence=0.6,
                    generated_at=datetime.now().isoformat(),
                    used_dummy_data=True,
                    matched_regulations=[],
                    fallback=True
                )
            )
    
    def _validate_input(self, request: AnalyticsRequest) -> bool:
        """å…¥åŠ›ã®è©³ç´°ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
        has_text = request.text and request.text.strip()
        has_files = request.files_content and any(content.strip() for content in request.files_content)
        
        if not has_text and not has_files:
            return False
        
        # ãƒ†ã‚­ã‚¹ãƒˆé•·åˆ¶é™ãƒã‚§ãƒƒã‚¯
        total_text_length = len(request.text or "")
        if request.files_content:
            total_text_length += sum(len(content) for content in request.files_content)
        
        if total_text_length > 50000:  # 50KBåˆ¶é™
            logger.warning(f"âš ï¸  ç·ãƒ†ã‚­ã‚¹ãƒˆé•·ãŒåˆ¶é™ã‚’è¶…ãˆã¦ã„ã¾ã™: {total_text_length} chars")
            return False
        
        return True
    
    def _preprocess_and_extract_features(self, request: AnalyticsRequest) -> Dict[str, Any]:
        """RAGé¢¨å‰å‡¦ç†ã¨ç‰¹å¾´æŠ½å‡º"""
        logger.info("ğŸ” ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã¨ç‰¹å¾´æŠ½å‡ºé–‹å§‹")
        
        # ãƒ†ã‚­ã‚¹ãƒˆçµ±åˆ
        combined_text = self._combine_text_sources(request.text, request.files_content or [])
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        keywords = self._extract_keywords(combined_text)
        
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
        entities = self._extract_entities(combined_text)
        
        # æ„å›³åˆ†é¡
        intent_category = self._classify_intent(combined_text, keywords)
        
        # è¤‡é›‘åº¦è©•ä¾¡
        complexity_score = self._assess_content_complexity(combined_text)
        
        processed_data = {
            "original_text": request.text,
            "files_content": request.files_content or [],
            "combined_text": combined_text,
            "keywords": keywords,
            "entities": entities,
            "intent_category": intent_category,
            "complexity_score": complexity_score,
            "text_length": len(combined_text),
            "file_count": len(request.files_content or [])
        }
        
        logger.info(f"âœ… ç‰¹å¾´æŠ½å‡ºå®Œäº†: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰{len(keywords)}å€‹, ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£{len(entities)}å€‹, è¤‡é›‘åº¦{complexity_score:.2f}")
        return processed_data
    
    def _combine_text_sources(self, text: str, files_content: List[str]) -> str:
        """è¤‡æ•°ã®æƒ…å ±æºã‚’é‡ã¿ä»˜ã‘ã—ã¦çµ±åˆ"""
        combined = ""
        
        # ãƒ¡ã‚¤ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆé‡ã¿: 1.0ï¼‰
        if text and text.strip():
            combined += text.strip()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ï¼ˆé‡ã¿: 0.8ï¼‰
        if files_content:
            for i, content in enumerate(files_content):
                if content.strip():
                    combined += f"\n\n[è³‡æ–™ {i+1}]\n{content.strip()}"
        
        return combined
    
    def _extract_keywords(self, text: str) -> List[str]:
        """é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æŠ½å‡º"""
        # æ¥­ç•Œç‰¹åŒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸
        industry_keywords = {
            "é…’é¡": ["é…’", "ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«", "é†¸é€ ", "è’¸ç•™", "ç™ºé…µ", "åº¦æ•°"],
            "é£Ÿå“": ["é£Ÿå“", "é£²æ–™", "è£½é€ ", "åŠ å·¥", "ä¿å­˜", "è³å‘³æœŸé™"],
            "æ³•ä»¤": ["æ³•å¾‹", "è¦åˆ¶", "å…è¨±", "è¨±å¯", "å±Šå‡º", "èªå¯"],
            "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°": ["åºƒå‘Š", "å®£ä¼", "è¡¨ç¤º", "ãƒ©ãƒ™ãƒ«", "è¨´æ±‚", "åŠ¹æœ"],
            "é–‹ç™º": ["æ–°å•†å“", "é–‹ç™º", "ä¼ç”»", "è¨­è¨ˆ", "è©¦ä½œ", "è©•ä¾¡"],
            "å“è³ª": ["å“è³ª", "å®‰å…¨", "æ¤œæŸ»", "åŸºæº–", "ç®¡ç†", "ä¿è¨¼"]
        }
        
        text_lower = text.lower()
        found_keywords = []
        
        for category, keywords in industry_keywords.items():
            for keyword in keywords:
                if keyword in text_lower:
                    found_keywords.append(f"{category}:{keyword}")
        
        # è¿½åŠ çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆé »å‡ºå˜èªï¼‰
        words = re.findall(r'\b[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼\w]+\b', text)
        word_freq = Counter(words)
        
        # é »å‡ºä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ 
        for word, freq in word_freq.most_common(10):
            if len(word) > 1 and freq > 1:
                found_keywords.append(f"é »å‡º:{word}")
        
        return found_keywords[:20]  # æœ€å¤§20å€‹ã¾ã§
    
    def _extract_entities(self, text: str) -> Dict[str, List[str]]:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        entities = {
            "products": [],
            "target_audience": [],
            "regulations": [],
            "departments": [],
            "timeframes": []
        }
        
        # å•†å“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹
        product_patterns = [
            r'æ–°[å•†å“|è£½å“|ã‚µãƒ¼ãƒ“ã‚¹]',
            r'[ä½|é«˜|ç„¡]ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«[å•†å“|é£²æ–™]?',
            r'æ©Ÿèƒ½æ€§[è¡¨ç¤º]?é£Ÿå“',
            r'å¥åº·[é£²æ–™|é£Ÿå“]'
        ]
        
        for pattern in product_patterns:
            matches = re.findall(pattern, text)
            entities["products"].extend(matches)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå±¤
        target_patterns = [
            r'\d+ä»£[ã®]?[ç”·æ€§|å¥³æ€§|å­¦ç”Ÿ|ç¤¾ä¼šäºº]',
            r'[è‹¥å¹´|ä¸­å¹´|é«˜é½¢][å±¤|è€…]',
            r'å¥åº·å¿—å‘[ã®]?[æ¶ˆè²»è€…|é¡§å®¢]'
        ]
        
        for pattern in target_patterns:
            matches = re.findall(pattern, text)
            entities["target_audience"].extend(matches)
        
        return entities
    
    def _classify_intent(self, text: str, keywords: List[str]) -> str:
        """ç›¸è«‡æ„å›³ã®åˆ†é¡"""
        intent_scores = {
            "æ–°å•†å“é–‹ç™º": 0,
            "æ³•ä»¤ç¢ºèª": 0,
            "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥": 0,
            "å“è³ªç®¡ç†": 0,
            "æµ·å¤–å±•é–‹": 0
        }
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        for keyword in keywords:
            if any(x in keyword for x in ["é–‹ç™º", "æ–°å•†å“", "ä¼ç”»"]):
                intent_scores["æ–°å•†å“é–‹ç™º"] += 1
            if any(x in keyword for x in ["æ³•å¾‹", "è¦åˆ¶", "å…è¨±"]):
                intent_scores["æ³•ä»¤ç¢ºèª"] += 1
            if any(x in keyword for x in ["åºƒå‘Š", "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "å®£ä¼"]):
                intent_scores["ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥"] += 1
            if any(x in keyword for x in ["å“è³ª", "å®‰å…¨", "æ¤œæŸ»"]):
                intent_scores["å“è³ªç®¡ç†"] += 1
            if any(x in keyword for x in ["æµ·å¤–", "è¼¸å‡º", "å›½éš›"]):
                intent_scores["æµ·å¤–å±•é–‹"] += 1
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®æ„å›³ã‚’è¿”ã™
        return max(intent_scores.items(), key=lambda x: x[1])[0]
    
    def _assess_content_complexity(self, text: str) -> float:
        """å†…å®¹ã®è¤‡é›‘åº¦è©•ä¾¡"""
        factors = {
            "length": min(len(text) / 1000, 1.0),  # ãƒ†ã‚­ã‚¹ãƒˆé•·
            "vocabulary": len(set(text.split())) / max(len(text.split()), 1),  # èªå½™ã®å¤šæ§˜æ€§
            "technical_terms": len([w for w in text.split() if len(w) > 6]) / max(len(text.split()), 1)  # å°‚é–€ç”¨èªã®å‰²åˆ
        }
        
        # é‡ã¿ä»˜ã‘å¹³å‡
        complexity = (
            factors["length"] * 0.4 +
            factors["vocabulary"] * 0.3 +
            factors["technical_terms"] * 0.3
        )
        
        return min(complexity, 1.0)
    
    async def _rag_reasoning_pipeline(self, processed_data: Dict[str, Any]) -> Dict[str, Any]:
        """RAGé¢¨å¤šæ®µéšæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        logger.info("ğŸ§  RAGæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
        
        # Step 1: é–¢é€£è¦åˆ¶ã®ç‰¹å®š
        relevant_regulations = self._identify_relevant_regulations(processed_data)
        
        # Step 2: è«–ç‚¹ã®æ§‹é€ åŒ–ç”Ÿæˆ
        structured_questions = self._generate_structured_questions(processed_data, relevant_regulations)
        
        # Step 3: æœ€é©ç›¸è«‡å…ˆã®é¸å®š
        optimal_consultants = self._select_optimal_consultants(processed_data, relevant_regulations)
        
        # Step 4: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆçµ±åˆ
        integrated_context = self._integrate_context(processed_data, relevant_regulations, structured_questions)
        
        # Step 5: å“è³ªè©•ä¾¡
        quality_metrics = self._evaluate_reasoning_quality(structured_questions, optimal_consultants)
        
        reasoning_result = {
            "regulations": relevant_regulations,
            "questions": structured_questions,
            "consultants": optimal_consultants,
            "context": integrated_context,
            "quality_metrics": quality_metrics
        }
        
        logger.info("âœ… RAGæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†")
        return reasoning_result
    
    def _identify_relevant_regulations(self, processed_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """é–¢é€£è¦åˆ¶ã®é«˜ç²¾åº¦ç‰¹å®š"""
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åŸºæœ¬çš„ãªåˆ†æã‚’å–å¾—
        base_analysis = self.dummy_service.analyze_consultation_content(
            text=processed_data["original_text"],
            files_content=processed_data["files_content"]
        )
        
        # è¿½åŠ çš„ãªæ¨è«–ã§ç²¾åº¦å‘ä¸Š
        enhanced_regulations = []
        for reg in base_analysis["analysis_metadata"].get("matched_regulations", []):
            reg_info = {
                "name": reg,
                "relevance_score": self._calculate_regulation_relevance(reg, processed_data),
                "key_points": self._get_regulation_key_points(reg, processed_data)
            }
            enhanced_regulations.append(reg_info)
        
        # é–¢é€£åº¦ã§ã‚½ãƒ¼ãƒˆ
        enhanced_regulations.sort(key=lambda x: x["relevance_score"], reverse=True)
        return enhanced_regulations[:3]  # ä¸Šä½3ã¤
    
    def _calculate_regulation_relevance(self, regulation: str, processed_data: Dict[str, Any]) -> float:
        """è¦åˆ¶ã®é–¢é€£åº¦è¨ˆç®—"""
        relevance_mapping = {
            "é…’ç¨æ³•": ["é…’", "ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«", "é†¸é€ ", "åº¦æ•°", "è£½é€ "],
            "é£Ÿå“è¡›ç”Ÿæ³•": ["é£Ÿå“", "å®‰å…¨", "è¡›ç”Ÿ", "è£½é€ ", "å“è³ª"],
            "æ™¯å“è¡¨ç¤ºæ³•": ["åºƒå‘Š", "è¡¨ç¤º", "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "åŠ¹æœ"],
            "è–¬æ©Ÿæ³•": ["å¥åº·", "æ©Ÿèƒ½æ€§", "åŠ¹èƒ½", "åŒ»è–¬"]
        }
        
        keywords = relevance_mapping.get(regulation, [])
        text_lower = processed_data["combined_text"].lower()
        
        matches = sum(1 for keyword in keywords if keyword in text_lower)
        return matches / len(keywords) if keywords else 0.0
    
    def _get_regulation_key_points(self, regulation: str, processed_data: Dict[str, Any]) -> List[str]:
        """è¦åˆ¶ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆæŠ½å‡º"""
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åŸºæœ¬ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—ã—ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¿œã˜ã¦èª¿æ•´
        regulation_data = next(
            (reg for reg in self.dummy_service.dummy_regulations if reg["category"] == regulation),
            None
        )
        
        if not regulation_data:
            return []
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¿œã˜ã¦ãƒã‚¤ãƒ³ãƒˆã‚’èª¿æ•´
        adjusted_points = []
        for point in regulation_data["points"]:
            # ç°¡æ˜“çš„ãªé–¢é€£åº¦ãƒã‚§ãƒƒã‚¯
            if any(keyword in processed_data["combined_text"].lower() 
                   for keyword in point.lower().split()):
                adjusted_points.append(point)
        
        return adjusted_points or regulation_data["points"][:2]  # æœ€ä½2ã¤ã¯ä¿è¨¼
    
    def _generate_structured_questions(self, processed_data: Dict[str, Any], regulations: List[Dict]) -> List[str]:
        """æ§‹é€ åŒ–ã•ã‚ŒãŸè³ªå•ç”Ÿæˆ"""
        questions = []
        
        # è¦åˆ¶ãƒ™ãƒ¼ã‚¹ã®è³ªå•
        for reg in regulations:
            for point in reg["key_points"]:
                context_question = f"{point}ã«ã¤ã„ã¦ã€{processed_data['intent_category']}ã®è¦³ç‚¹ã‹ã‚‰å…·ä½“çš„ãªå¯¾å¿œæ–¹é‡"
                questions.append(context_question)
        
        # æ„å›³ãƒ™ãƒ¼ã‚¹ã®è¿½åŠ è³ªå•
        intent_specific_questions = self._generate_intent_specific_questions(processed_data)
        questions.extend(intent_specific_questions)
        
        # é‡è¤‡æ’é™¤ã¨å„ªå…ˆé †ä½ä»˜ã‘
        unique_questions = list(dict.fromkeys(questions))
        return unique_questions[:5]  # æœ€å¤§5ã¤
    
    def _generate_intent_specific_questions(self, processed_data: Dict[str, Any]) -> List[str]:
        """æ„å›³åˆ¥ç‰¹åŒ–è³ªå•"""
        intent = processed_data["intent_category"]
        
        intent_questions = {
            "æ–°å•†å“é–‹ç™º": [
                "å•†å“ã‚³ãƒ³ã‚»ãƒ—ãƒˆã®æ³•çš„å®Ÿç¾å¯èƒ½æ€§ã®è©•ä¾¡",
                "è£½é€ ãƒ—ãƒ­ã‚»ã‚¹ã«ãŠã‘ã‚‹è¦åˆ¶å¯¾å¿œã®å…·ä½“çš„æ‰‹é †"
            ],
            "æ³•ä»¤ç¢ºèª": [
                "è©²å½“æ³•ä»¤ã®è©³ç´°ãªé©ç”¨ç¯„å›²ã®ç¢ºèª",
                "å¿…è¦ãªç”³è«‹ãƒ»å±Šå‡ºæ‰‹ç¶šãã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³"
            ],
            "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥": [
                "åºƒå‘Šè¡¨ç¾ã®æ³•çš„ãƒªã‚¹ã‚¯ã‚¢ã‚»ã‚¹ãƒ¡ãƒ³ãƒˆ",
                "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨´æ±‚ã«ãŠã‘ã‚‹è¦åˆ¶åˆ¶ç´„ã®æ•´ç†"
            ]
        }
        
        return intent_questions.get(intent, [])
    
    def _select_optimal_consultants(self, processed_data: Dict[str, Any], regulations: List[Dict]) -> List[Dict]:
        """æœ€é©ç›¸è«‡å…ˆã®é«˜ç²¾åº¦é¸å®š"""
        # åŸºæœ¬çš„ãªç›¸è«‡å…ˆé¸å®š
        base_analysis = self.dummy_service.analyze_consultation_content(
            text=processed_data["original_text"],
            files_content=processed_data["files_content"]
        )
        
        base_consultants = base_analysis["consultants"]
        
        # è¿½åŠ çš„ãªæœ€é©åŒ–
        optimized_consultants = []
        for consultant in base_consultants:
            consultant_score = self._calculate_consultant_match_score(
                consultant, processed_data, regulations
            )
            consultant["match_score"] = consultant_score
            optimized_consultants.append(consultant)
        
        # ãƒãƒƒãƒã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        optimized_consultants.sort(key=lambda x: x.get("match_score", 0), reverse=True)
        return optimized_consultants[:3]  # ä¸Šä½3å
    
    def _calculate_consultant_match_score(self, consultant: Dict, processed_data: Dict, regulations: List[Dict]) -> float:
        """ç›¸è«‡å…ˆãƒãƒƒãƒã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 0.0
        
        # å°‚é–€åˆ†é‡ã®ä¸€è‡´åº¦
        expertise_keywords = consultant["expertise"].lower().split("ãƒ»")
        for reg in regulations:
            if any(keyword in reg["name"].lower() for keyword in expertise_keywords):
                score += reg["relevance_score"] * 0.4
        
        # æ„å›³ã‚«ãƒ†ã‚´ãƒªã¨ã®ä¸€è‡´åº¦
        intent = processed_data["intent_category"]
        department_intent_mapping = {
            "æ³•å‹™éƒ¨": ["æ³•ä»¤ç¢ºèª"],
            "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°éƒ¨": ["ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥"],
            "å•†å“é–‹ç™ºéƒ¨": ["æ–°å•†å“é–‹ç™º"],
            "å“è³ªä¿è¨¼éƒ¨": ["å“è³ªç®¡ç†"]
        }
        
        if intent in department_intent_mapping.get(consultant["department"], []):
            score += 0.3
        
        return min(score, 1.0)
    
    def _integrate_context(self, processed_data: Dict, regulations: List[Dict], questions: List[str]) -> Dict[str, Any]:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆçµ±åˆ"""
        return {
            "primary_regulations": [reg["name"] for reg in regulations],
            "content_complexity": processed_data["complexity_score"],
            "question_count": len(questions),
            "has_file_content": len(processed_data["files_content"]) > 0,
            "intent_confidence": 0.8  # æ„å›³åˆ†é¡ã®ä¿¡é ¼åº¦
        }
    
    def _evaluate_reasoning_quality(self, questions: List[str], consultants: List[Dict]) -> Dict[str, float]:
        """æ¨è«–å“è³ªã®è©•ä¾¡"""
        return {
            "question_diversity": len(set(q[:20] for q in questions)) / max(len(questions), 1),
            "consultant_coverage": len(set(c["department"] for c in consultants)) / max(len(consultants), 1),
            "overall_completeness": min((len(questions) + len(consultants)) / 8, 1.0)
        }
    
    def _calculate_dynamic_confidence(self, processed_data: Dict, reasoning_result: Dict) -> float:
        """å‹•çš„ä¿¡é ¼åº¦è¨ˆç®—"""
        confidence_factors = {
            "keyword_coverage": len(processed_data["keywords"]) / 10,  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç¶²ç¾…åº¦
            "regulation_relevance": sum(reg["relevance_score"] for reg in reasoning_result["regulations"]) / len(reasoning_result["regulations"]),
            "consultant_match": sum(c.get("match_score", 0) for c in reasoning_result["consultants"]) / len(reasoning_result["consultants"]),
            "content_quality": processed_data["complexity_score"],
            "reasoning_quality": reasoning_result["quality_metrics"]["overall_completeness"]
        }
        
        # é‡ã¿ä»˜ã‘è¨ˆç®—
        weighted_confidence = sum(
            confidence_factors[factor] * weight 
            for factor, weight in self.confidence_weights.items() 
            if factor in confidence_factors
        )
        
        return min(max(weighted_confidence, 0.0), 1.0)
    
    def _generate_reasoning_trace(self, processed_data: Dict, reasoning_result: Dict) -> Dict[str, Any]:
        """æ¨è«–éç¨‹ã®è¨˜éŒ²"""
        return {
            "input_analysis": {
                "text_length": processed_data["text_length"],
                "keywords_found": len(processed_data["keywords"]),
                "intent_detected": processed_data["intent_category"]
            },
            "regulation_matching": {
                "regulations_identified": len(reasoning_result["regulations"]),
                "max_relevance_score": max((reg["relevance_score"] for reg in reasoning_result["regulations"]), default=0)
            },
            "consultant_selection": {
                "consultants_evaluated": len(reasoning_result["consultants"]),
                "selection_criteria": "relevance_score + department_match + expertise_alignment"
            },
            "reasoning_path": f"Intent({processed_data['intent_category']}) â†’ Regulations({len(reasoning_result['regulations'])}) â†’ Questions({len(reasoning_result['questions'])}) â†’ Consultants({len(reasoning_result['consultants'])})"
        }
    
    def _build_final_response(self, reasoning_result: Dict, confidence_score: float, reasoning_trace: Dict) -> AnalyticsResponse:
        """æœ€çµ‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹ç¯‰"""
        # ã‚µãƒãƒªãƒ¼ç”Ÿæˆï¼ˆã‚ˆã‚Šè©³ç´°ãªæ¨è«–ãƒ™ãƒ¼ã‚¹ï¼‰
        regulations_text = "ã€".join(reg["name"] for reg in reasoning_result["regulations"])
        summary = f"æå‡ºã•ã‚ŒãŸä¼ç”»æ¡ˆã«ã¤ã„ã¦ã€{regulations_text}ã®è¦³ç‚¹ã‹ã‚‰è©³ç´°ãªæ¤œè¨ãŒå¿…è¦ã§ã™ã€‚ç‰¹ã«{reasoning_result['context']['primary_regulations'][0]}ã¸ã®å¯¾å¿œãŒé‡è¦ã¨åˆ¤æ–­ã•ã‚Œã¾ã™ã€‚"
        
        return AnalyticsResponse(
            summary=summary,
            questions=reasoning_result["questions"],
            consultants=[
                ConsultantInfo(
                    name=consultant["name"],
                    department=consultant["department"],
                    expertise=consultant["expertise"]
                )
                for consultant in reasoning_result["consultants"]
            ],
            analysis_metadata=AnalysisMetadata(
                confidence=confidence_score,
                generated_at=datetime.now().isoformat(),
                used_dummy_data=True,
                matched_regulations=reasoning_result["context"]["primary_regulations"],
                fallback=False
            )
        )
    
    async def get_dummy_data_info(self) -> dict:
        """
        ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã‚’å–å¾—ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        
        ğŸš¨ æœ¬ç•ªç’°å¢ƒã§ã¯å‰Šé™¤äºˆå®š
        """
        base_info = self.dummy_service.get_dummy_data_info()
        base_info.update({
            "rag_features": {
                "multi_stage_reasoning": True,
                "dynamic_confidence": True,
                "context_integration": True,
                "reasoning_trace": True
            },
            "enhancement_level": "RAG-equivalent"
        })
        return base_info