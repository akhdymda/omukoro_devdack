import logging
import numpy as np
from typing import List, Dict, Any, Tuple
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from openai import AsyncOpenAI
import json
import os
from datetime import datetime
from app.services.dummy_data_service import DummyDataService

logger = logging.getLogger(__name__)

class RAGService:
    """
    æœ¬æ ¼çš„ãªRAGã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…
    
    - ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã«ã‚ˆã‚‹é¡ä¼¼åº¦æ¤œç´¢
    - OpenAI APIã¨ã®çµ±åˆ
    - ç›¸è«‡å…ˆã¨è³ªå•äº‹é …ã®é–¢é€£ä»˜ã‘
    """
    
    def __init__(self):
        # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        try:
            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
            logger.info("âœ… SentenceTransformer model loaded successfully")
        except Exception as e:
            logger.error(f"âŒ Failed to load embedding model: {e}")
            self.embedding_model = None
        
        # OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            self.openai_client = AsyncOpenAI(api_key=api_key)
            logger.info("âœ… OpenAI client initialized")
        else:
            self.openai_client = None
            logger.warning("âš ï¸ OpenAI API key not found")
        
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ï¼‰
        self.dummy_service = DummyDataService()
        
        # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        self.knowledge_vectors = None
        self.knowledge_items = []
        self._initialize_knowledge_base()
    
    def _initialize_knowledge_base(self):
        """çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ï¼ˆãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼‰ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        try:
            logger.info("ğŸ§  çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­...")
            
            # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çŸ¥è­˜é …ç›®ã‚’æŠ½å‡º
            self.knowledge_items = []
            
            # è¦åˆ¶ãƒ»æ³•ä»¤æƒ…å ±
            for regulation in self.dummy_service.dummy_regulations:
                for point in regulation["points"]:
                    self.knowledge_items.append({
                        "type": "regulation",
                        "category": regulation["category"],
                        "content": point,
                        "related_consultants": self._get_related_consultants(regulation["category"])
                    })
            
            # éå»äº‹ä¾‹æƒ…å ±
            for case in self.dummy_service.dummy_cases:
                for question in case["common_questions"]:
                    self.knowledge_items.append({
                        "type": "case",
                        "keywords": case["keywords"],
                        "content": question,
                        "related_consultants": self._get_consultants_by_keywords(case["keywords"])
                    })
            
            # ç›¸è«‡å…ˆæƒ…å ±
            for consultant in self.dummy_service.dummy_consultants:
                expertise_text = f"{consultant['department']}ã®{consultant['expertise']}ã«é–¢ã™ã‚‹å°‚é–€çŸ¥è­˜"
                self.knowledge_items.append({
                    "type": "consultant",
                    "content": expertise_text,
                    "consultant_info": consultant,
                    "related_consultants": [consultant]
                })
            
            # ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Ÿè¡Œ
            if self.embedding_model:
                contents = [item["content"] for item in self.knowledge_items]
                self.knowledge_vectors = self.embedding_model.encode(contents)
                logger.info(f"âœ… {len(self.knowledge_items)}ä»¶ã®çŸ¥è­˜é …ç›®ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†")
            else:
                logger.warning("âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                
        except Exception as e:
            logger.error(f"âŒ çŸ¥è­˜ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _get_related_consultants(self, regulation_category: str) -> List[Dict]:
        """è¦åˆ¶ã‚«ãƒ†ã‚´ãƒªã«é–¢é€£ã™ã‚‹ç›¸è«‡å…ˆã‚’å–å¾—"""
        mapping = {
            "é…’ç¨æ³•": ["ç”°ä¸­ æ³•å‹™"],
            "é£Ÿå“è¡›ç”Ÿæ³•": ["ç”°ä¸­ æ³•å‹™", "ä½è—¤ å“è³ª"],
            "æ™¯å“è¡¨ç¤ºæ³•": ["å±±ç”° ãƒãƒ¼ã‚±"],
            "è–¬æ©Ÿæ³•": ["ç”°ä¸­ æ³•å‹™", "å±±ç”° ãƒãƒ¼ã‚±"]
        }
        
        consultant_names = mapping.get(regulation_category, [])
        consultants = []
        
        for name in consultant_names:
            consultant = next(
                (c for c in self.dummy_service.dummy_consultants if c["name"] == name),
                None
            )
            if consultant:
                consultants.append(consultant)
        
        return consultants
    
    def _get_consultants_by_keywords(self, keywords: List[str]) -> List[Dict]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦ç›¸è«‡å…ˆã‚’æ¨å®š"""
        consultant_mapping = {
            "æ–°å•†å“": "éˆ´æœ¨ é–‹ç™º",
            "ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«": "ç”°ä¸­ æ³•å‹™",
            "å¥åº·": "å±±ç”° ãƒãƒ¼ã‚±",
            "æµ·å¤–": "é«˜æ©‹ å–¶æ¥­"
        }
        
        consultants = []
        for keyword in keywords:
            name = consultant_mapping.get(keyword)
            if name:
                consultant = next(
                    (c for c in self.dummy_service.dummy_consultants if c["name"] == name),
                    None
                )
                if consultant and consultant not in consultants:
                    consultants.append(consultant)
        
        return consultants[:2]  # æœ€å¤§2å
    
    async def analyze_with_rag(self, text: str, files_content: List[str] = None) -> Dict[str, Any]:
        """
        RAGã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªåˆ†æå‡¦ç†
        
        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            files_content: ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            Dict: RAGåˆ†æçµæœ
        """
        try:
            logger.info("ğŸš€ RAGåˆ†æã‚’é–‹å§‹")
            
            # Step 1: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®çµ±åˆã¨å‰å‡¦ç†
            combined_text = self._combine_input_texts(text, files_content or [])
            
            # Step 2: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            query_vector = self._vectorize_query(combined_text)
            
            # Step 3: é¡ä¼¼åº¦æ¤œç´¢ã§é–¢é€£çŸ¥è­˜ã‚’å–å¾—
            relevant_knowledge = self._search_similar_knowledge(query_vector, top_k=5)
            
            # Step 4: LLMã‚’ä½¿ç”¨ã—ãŸè«–ç‚¹æ•´ç†
            llm_analysis = await self._llm_reasoning(combined_text, relevant_knowledge)
            
            # Step 5: ç›¸è«‡å…ˆã¨è³ªå•ã®é–¢é€£ä»˜ã‘
            structured_result = self._structure_result_with_consultants(llm_analysis, relevant_knowledge)
            
            logger.info("âœ… RAGåˆ†æå®Œäº†")
            return structured_result
            
        except Exception as e:
            logger.error(f"âŒ RAGåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¾“æ¥ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿åˆ†æ
            return self.dummy_service.analyze_consultation_content(text, files_content)
    
    def _combine_input_texts(self, text: str, files_content: List[str]) -> str:
        """å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’çµ±åˆ"""
        combined = text.strip()
        
        if files_content:
            for i, content in enumerate(files_content):
                if content.strip():
                    combined += f"\n\n[æ·»ä»˜è³‡æ–™ {i+1}]\n{content.strip()}"
        
        return combined
    
    def _vectorize_query(self, text: str) -> np.ndarray:
        """ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        if not self.embedding_model:
            return np.array([])
        
        try:
            vector = self.embedding_model.encode([text])
            return vector[0]
        except Exception as e:
            logger.error(f"âŒ ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return np.array([])
    
    def _search_similar_knowledge(self, query_vector: np.ndarray, top_k: int = 5) -> List[Dict]:
        """é¡ä¼¼åº¦æ¤œç´¢ã§é–¢é€£çŸ¥è­˜ã‚’å–å¾—"""
        if self.knowledge_vectors is None or len(query_vector) == 0:
            logger.warning("âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ©ãƒ³ãƒ€ãƒ é¸æŠã—ã¾ã™ã€‚")
            return self.knowledge_items[:top_k]
        
        try:
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
            similarities = cosine_similarity([query_vector], self.knowledge_vectors)[0]
            
            # é¡ä¼¼åº¦ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
            top_indices = np.argsort(similarities)[::-1][:top_k]
            
            relevant_items = []
            for idx in top_indices:
                item = self.knowledge_items[idx].copy()
                item["similarity_score"] = float(similarities[idx])
                relevant_items.append(item)
            
            logger.info(f"ğŸ” é¡ä¼¼åº¦æ¤œç´¢å®Œäº†: {len(relevant_items)}ä»¶ã®é–¢é€£çŸ¥è­˜ã‚’å–å¾—")
            return relevant_items
            
        except Exception as e:
            logger.error(f"âŒ é¡ä¼¼åº¦æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return self.knowledge_items[:top_k]
    
    async def _llm_reasoning(self, query_text: str, relevant_knowledge: List[Dict]) -> Dict[str, Any]:
        """LLMã‚’ä½¿ç”¨ã—ãŸè«–ç‚¹æ•´ç†"""
        if not self.openai_client:
            logger.warning("âš ï¸ OpenAI APIãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return self._fallback_reasoning(query_text, relevant_knowledge)
        
        try:
            # é–¢é€£çŸ¥è­˜ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«çµ„ã¿è¾¼ã¿
            knowledge_context = self._build_knowledge_context(relevant_knowledge)
            
            system_prompt = """ã‚ãªãŸã¯é…’é¡æ¥­ç•Œã®æ³•å‹™ãƒ»è¦åˆ¶ã®å°‚é–€å®¶ã§ã™ã€‚
ç›¸è«‡å†…å®¹ã«å¯¾ã—ã¦ã€é–¢é€£ã™ã‚‹æ³•ä»¤ãƒ»è¦åˆ¶ã‚’å‚ç…§ã—ã€å…·ä½“çš„ãªè«–ç‚¹ã‚’æ•´ç†ã—ã¦ãã ã•ã„ã€‚
å¿…ãšJSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""

            user_prompt = f"""
ã€ç›¸è«‡å†…å®¹ã€‘
{query_text}

ã€å‚è€ƒçŸ¥è­˜ã€‘
{knowledge_context}

ä»¥ä¸‹ã®å½¢å¼ã§JSONå›ç­”ã—ã¦ãã ã•ã„ï¼š
{{
    "summary": "ç›¸è«‡å†…å®¹ã®è¦ç´„ï¼ˆ50æ–‡å­—ç¨‹åº¦ï¼‰",
    "questions": [
        {{
            "question": "å…·ä½“çš„ãªè³ªå•ã‚„è«–ç‚¹",
            "category": "é–¢é€£ã™ã‚‹è¦åˆ¶ã‚«ãƒ†ã‚´ãƒª",
            "priority": "high/medium/low",
            "related_knowledge_ids": [å‚è€ƒã«ã—ãŸçŸ¥è­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹]
        }}
    ],
    "confidence": 0.0-1.0ã®ä¿¡é ¼åº¦,
    "reasoning": "åˆ¤å®šç†ç”±ï¼ˆ100æ–‡å­—ä»¥å†…ï¼‰"
}}
"""
            
            response = await self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=800,
                temperature=0.3,
                timeout=15
            )
            
            content = response.choices[0].message.content
            llm_result = json.loads(content)
            
            logger.info("âœ… LLMè«–ç‚¹æ•´ç†å®Œäº†")
            return llm_result
            
        except Exception as e:
            logger.error(f"âŒ LLMæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            return self._fallback_reasoning(query_text, relevant_knowledge)
    
    def _build_knowledge_context(self, relevant_knowledge: List[Dict]) -> str:
        """é–¢é€£çŸ¥è­˜ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã«æ§‹ç¯‰"""
        context_parts = []
        
        for i, item in enumerate(relevant_knowledge):
            similarity = item.get("similarity_score", 0)
            content_type = item["type"]
            content = item["content"]
            
            if content_type == "regulation":
                category = item["category"]
                context_parts.append(f"[{i}] ã€{category}ã€‘{content} (é–¢é€£åº¦: {similarity:.2f})")
            elif content_type == "case":
                keywords = ", ".join(item["keywords"])
                context_parts.append(f"[{i}] ã€éå»äº‹ä¾‹: {keywords}ã€‘{content} (é–¢é€£åº¦: {similarity:.2f})")
            elif content_type == "consultant":
                context_parts.append(f"[{i}] ã€å°‚é–€çŸ¥è­˜ã€‘{content} (é–¢é€£åº¦: {similarity:.2f})")
        
        return "\n".join(context_parts)
    
    def _fallback_reasoning(self, query_text: str, relevant_knowledge: List[Dict]) -> Dict[str, Any]:
        """LLMåˆ©ç”¨ä¸å¯æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¨è«–"""
        questions = []
        categories = set()
        
        # é–¢é€£çŸ¥è­˜ã‹ã‚‰è³ªå•ã‚’ç”Ÿæˆ
        for item in relevant_knowledge[:3]:
            if item["type"] == "regulation":
                question = f"{item['category']}ã«ãŠã‘ã‚‹{item['content']}ã«ã¤ã„ã¦"
                questions.append({
                    "question": question,
                    "category": item["category"],
                    "priority": "medium",
                    "related_knowledge_ids": [relevant_knowledge.index(item)]
                })
                categories.add(item["category"])
        
        return {
            "summary": f"æå‡ºã•ã‚ŒãŸä¼ç”»æ¡ˆã«ã¤ã„ã¦ã€{', '.join(categories)}ã®è¦³ç‚¹ã‹ã‚‰æ¤œè¨ãŒå¿…è¦ã§ã™ã€‚",
            "questions": questions,
            "confidence": 0.75,
            "reasoning": "é–¢é€£çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®é¡ä¼¼åº¦æ¤œç´¢çµæœã«ã‚ˆã‚‹æ¨è«–"
        }
    
    def _structure_result_with_consultants(self, llm_analysis: Dict, relevant_knowledge: List[Dict]) -> Dict[str, Any]:
        """ç›¸è«‡å…ˆã¨è³ªå•äº‹é …ã‚’é–¢é€£ä»˜ã‘ãŸçµæœæ§‹é€ ã‚’ä½œæˆ"""
        try:
            # è³ªå•äº‹é …ã‹ã‚‰ç›¸è«‡å…ˆã‚’æ¨å®š
            consultants_map = {}
            question_consultant_mapping = []
            
            for question_data in llm_analysis.get("questions", []):
                question_text = question_data["question"]
                category = question_data.get("category", "")
                related_ids = question_data.get("related_knowledge_ids", [])
                
                # é–¢é€£çŸ¥è­˜ã‹ã‚‰ç›¸è«‡å…ˆã‚’ç‰¹å®š
                relevant_consultants = []
                for knowledge_id in related_ids:
                    if knowledge_id < len(relevant_knowledge):
                        knowledge = relevant_knowledge[knowledge_id]
                        if "related_consultants" in knowledge:
                            relevant_consultants.extend(knowledge["related_consultants"])
                
                # é‡è¤‡æ’é™¤
                unique_consultants = []
                for consultant in relevant_consultants:
                    if consultant not in unique_consultants:
                        unique_consultants.append(consultant)
                        consultants_map[consultant["name"]] = consultant
                
                question_consultant_mapping.append({
                    "question": question_text,
                    "category": category,
                    "priority": question_data.get("priority", "medium"),
                    "consultants": [c["name"] for c in unique_consultants[:2]]  # æœ€å¤§2å
                })
            
            # æœ€çµ‚çš„ãªç›¸è«‡å…ˆãƒªã‚¹ãƒˆï¼ˆé‡è¤‡æ’é™¤æ¸ˆã¿ï¼‰
            final_consultants = list(consultants_map.values())[:3]  # æœ€å¤§3å
            
            # çµæœæ§‹é€ ä½œæˆ
            return {
                "summary": llm_analysis.get("summary", "ä¼ç”»æ¡ˆã«ã¤ã„ã¦æ³•ä»¤é©åˆæ€§ã®ç¢ºèªãŒå¿…è¦ã§ã™ã€‚"),
                "questions": [mapping["question"] for mapping in question_consultant_mapping],
                "consultants": final_consultants,
                "question_consultant_mapping": question_consultant_mapping,  # ğŸ†• è¿½åŠ 
                "analysis_metadata": {
                    "confidence": llm_analysis.get("confidence", 0.8),
                    "generated_at": datetime.now().isoformat(),
                    "used_dummy_data": True,  # ç¾åœ¨ã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
                    "rag_enabled": True,
                    "llm_reasoning": bool(self.openai_client),
                    "vector_search": bool(self.embedding_model),
                    "knowledge_items_used": len(relevant_knowledge)
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ çµæœæ§‹é€ åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            # åŸºæœ¬çš„ãªæ§‹é€ ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return {
                "summary": llm_analysis.get("summary", "ä¼ç”»æ¡ˆã«ã¤ã„ã¦æ¤œè¨ãŒå¿…è¦ã§ã™ã€‚"),
                "questions": [q.get("question", q) if isinstance(q, dict) else q for q in llm_analysis.get("questions", [])],
                "consultants": final_consultants if 'final_consultants' in locals() else [],
                "question_consultant_mapping": [],
                "analysis_metadata": {
                    "confidence": 0.6,
                    "generated_at": datetime.now().isoformat(),
                    "used_dummy_data": True,
                    "rag_enabled": True,
                    "fallback": True
                }
            }
    
    def get_service_info(self) -> Dict[str, Any]:
        """ã‚µãƒ¼ãƒ“ã‚¹æƒ…å ±ã‚’å–å¾—ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        return {
            "service": "RAGService",
            "version": "1.0.0",
            "features": {
                "vector_search": bool(self.embedding_model),
                "llm_reasoning": bool(self.openai_client),
                "knowledge_base_size": len(self.knowledge_items),
                "embedding_model": "all-MiniLM-L6-v2" if self.embedding_model else None
            },
            "status": "production-ready"
        }